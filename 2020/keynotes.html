---
layout: default2020
---

<h3><a name="KeynoteML">Keynote<br>Ming Lin, University of Maryland</a></h3>

<div style="display: block; margin-left: 20px; width: 40%;">
    <p class="text-center" align="center"><img src="img/Ming_Lin_UMIACS.jpg" width="289"></p>
</div>
<div style="display: block; margin-left: 20px; width: 80%; vertical-align: top">

 <p><b>Title:</b> Reconstructing Reality: From Physical World to Virtual Environments
	<!--	
	<a href="keynotes/I3D2020_keynote_MingLin.pdf">PDF</a></p>
	 -->
	<p><b>Abstract:</b>
		With increasing availability of data in various forms from images, audio, video, 3D models, motion capture,
		simulation results, to satellite imagery, representative samples of the various phenomena constituting the world
		around us bring new opportunities and research challenges. Such availability of data has led to recent advances
		in data-driven modeling. However, most of the existing example-based synthesis methods offer empirical
		models and data reconstruction that may not provide an insightful understanding of the underlying process or
		may be limited to a subset of observations.</p>
	<p>
		In this talk, I present recent advances that integrate classical model-based methods and statistical learning
		techniques to tackle challenging problems that have not been previously addressed. These include flow
		reconstruction for traffic visualization, learning heterogeneous crowd behaviors from video, simultaneous
		estimation of deformation and elasticity parameters from images and video, and example-based multimodal
		display for VR systems. These approaches offer new insights for understanding complex collective behaviors,
		developing better models for complex dynamical systems from captured data, delivering more effective medical
		diagnosis and treatment, as well as cyber-manufacturing of customized apparel. I conclude by discussing some
		possible future directions and challenges.
	</p>


	<p><b>Bio:</b> <a href="https://www.cs.umd.edu/~lin/">Ming Lin</a> is an American computer scientist and the chair of the Department of Computer Science at the University of Maryland, College Park, where she also holds an endowed faculty position as the Elizabeth Stevinson Iribe Chair of Computer Science. Prior to moving to Maryland in 2018, Lin was the John R. &amp; Louise S. Parker Distinguished Professor of Computer Science at the University of North Carolina at Chapel Hill.</p>
	
	<p>Lin is known for her work on collision detection, and in particular for the Lin–Canny algorithm for maintaining the closest pair of features of two moving objects, for the idea (with Cohen, Manocha, and Ponamgi) of using axis-aligned bounding boxes to quickly eliminate from consideration pairs of objects that are far from colliding, and for additional speedups to collision detection using bounding box hierarchies. Her software libraries implementing these algorithms are widely used in commercial applications including computer aided design and computer games. More generally, her research interests are in physically based modeling, haptics, robotics, 3D computer graphics, computational geometry, and interactive computer simulation.</p>
	
</div>

<h3><a name="KeynoteJP">Keynote<br>Julien Pettré, Inria</a></h3>

<div style="display: block; margin-left: 20px; width: 40%;">
    <p class="text-center" align="center"><img src="img/Julien.png" width="289"></p>
</div>
<div style="display: block; margin-left: 20px; width: 80%; vertical-align: top">

<!--	
<p><b>Title:</b>Here
<a href="keynotes/I3D2020_keynote_MingLin.pdf">PDF</a></p>
	
    <p><b>Abstract:</b> 
</p>
 -->
 
	<p><b>Bio:</b> <a href="https://team.inria.fr/rainbow/fr/team/julien-pettre/">Julien Pettré</a> is a computer scientist. He is senior researcher at Inria, the French National Institute for Research in Computer Science and Control, in the Rainbow team. He received PhD from the University of Toulouse III in 2003, and Habilitation from the University of Rennes I in 2015. From 2004 to 2006, he was postdoctoral fellow at EPFL in Switzerland. He joined Inria in 2006.</p> 
		
	<p>Julien Pettré is currently coordinator of the European H2020 Crowdbot project (2018-21) dedicated to the design of robot navigation techniques for crowded environments. He is also the coordinator of the European H2020 Fet Open CrowdDNA project (2020-2024), dedicated to future emergent technologies for crowd management in public spaces. He previously coordinated the national ANR JCJC Percolation project (2013-17) dedicated to the design of new microscopic crowd simulation algorithms, as well as the national ANR CONTINT Chrome project, dedicated to efficient and designer-friendly techniques for crowd animation. His research interests are crowd simulation, computer animation, virtual reality, robot navigation and motion planning.
	</p>
	
</div>


<h3><a name="KeynoteRRNH">Keynote<br>Rachel Rose, ILM</a></h3>

<div style="display: block; margin-left: 20px; width: 40%;">
    <p class="text-center" align="center"><img src="img/Rachel_Rose.jpg" width="289"></p>
</div>
<div style="display: block; margin-left: 20px; width: 80%; vertical-align: top">

<!--	
<p><b>Title:</b>Here
<a href="keynotes/I3D2020_keynote_NatyHoffman.pdf">PDF</a></p>
	
    <p><b>Abstract:</b> 
</p>
 -->
 
	<p><b>Bio:</b> <a href="https://www.linkedin.com/in/rachel-rose-6905a12/">Rachel Rose</a>, an R&amp;D Supervisor at Industrial Light &amp; Magic (ILM), drives technology that aids artists in the creation and animation of characters for feature films. Rachel has a strong interest in the intersection of real-time technology and the film industry that started with her Ph.D. thesis on real-time motion synthesis. In her 12 years at ILM, Rachel has worked on a wide range of films, TV shows, and video games, including <I>Rango</I> (2011), <I>Noah</I> (2014), <I>Rogue One: A Star Wars Story (2016)</I>, <I>Star Wars: The Force Unleashed 2 (2010)</I>, and, most recently, as ILM Technology Supervisor for <I>The Mandalorian</I> (2019). Rachel is a member of the Academy of Motion Picture Arts and Sciences (AMPAS), and her work on BlockParty, a visual, procedural rigging system, earned her a Technical Achievement Award from AMPAS in 2018.</p>
	
</div>

<h3><a name="KeynoteRRNH">Keynote<br>Naty Hoffman, Lucasfilm</a></h3>

<div style="display: block; margin-left: 20px; width: 40%;">
    <p class="text-center" align="center"><img src="img/Naty_Hoffman.jpg" width="289"></p>
</div>
<div style="display: block; margin-left: 20px; width: 80%; vertical-align: top">
	
<p><b>Title:</b> Real-Time Rendering At Lucasfilm: Working with The Coolest Toys In The Galaxy
<!--
<a href="keynotes/I3D2020_keynote_NatyHoffman.pdf">PDF</a></p>
-->
	
	<p><b>Abstract:</b> From 1980's point-and-click adventure games to the ILM StageCraft tool suite used for <I>The Mandalorian</I> television series, real-time rendering has been woven throughout Lucasfilm's history. The mix of game technology and feature film visual effects expertise has produced many unique projects at Lucasfilm, including the interactive theme-park ride <I>Star Wars: Millennium Falcon – Smugglers Run</I>, the Academy Award-winning virtual reality experience <I>Carne y Arena</I>, and the groundbreaking advances in virtual production culminating in capturing final pixels in camera using ILM StageCraft on <I>The Mandalorian</I>. We will discuss the technical challenges we faced on these projects, as well as the opportunity they afforded us to work with some of the coolest toys in the galaxy: eight-GPU monster PCs, pre-release virtual reality headsets, custom motion capture hardware, 20' high wraparound LED displays, and feature film assets from some of the world's most beloved media properties.
</p>

	<p><b>Bio:</b> <a href="https://www.linkedin.com/in/natyhoffman/">Naty Hoffman</a>
	is a Principal Engineer &amp; Architect in the Advanced Development Group at Lucasfilm,
	where he has helped drive real-time rendering technology on multiple projects including <I>The Mandalorian</I>
	TV show, the <I>Smuggler’s Run</I> theme park ride, and the Virtual Reality experiences <I>Secrets of the Empire</I>
	and <I>Vader Immortal</I>. Naty joined Lucasfilm in early 2016 after two decades in the game industry, during
	which he played a key role in bringing advances such as physically based shading and cinematic color
	grading to game production. Naty's contributions include graphics technology for celebrated game
	franchises such as <I>Call of Duty</I> and <I>God of War</I>, core technology libraries for Sony's Playstation 3
	console, and influential publications including the <I>Physics and Math of Shading</I> talks at SIGGRAPH and
	the book <I>Real-Time Rendering</I>.</p>
	
</div>



<h3><a name="KeynoteRRNH">Keynote<br>David Morin, Epic Games</a></h3>

<div style="display: block; margin-left: 20px; width: 40%;">
    <p class="text-center" align="center"><img src="img/David_Morin.jpg" width="289"></p>
</div>
<div style="display: block; margin-left: 20px; width: 80%; vertical-align: top">

<p><b>Title:</b> Creating In-Camera VFX with Real-Time Workflows
<!--	
<a href="keynotes/I3D2020_keynote_DavidMorin.pdf">PDF</a></p>
 -->
<p><b>Abstract:</b>
	This session will cover advancements in “in-camera visual effects” and how this technique is changing the film and TV industry. With software developments in real-time game engines, combined with hardware developments in GPUs and on-set video equipment, filmmakers can now capture final pixel visual effects while still on set – enabling new levels of creative collaboration and efficiency during principal photography. These new developments allow changes to digital scenes, even those at final pixel quality, to be seen instantly on high-resolution LED walls – an exponential degree of time savings over a traditional CG rendering workflow. This is crucial as there is a huge demand for more original film and TV content, and studios must find a way to efficiently scale production and post-production while maintaining high quality and creative intent.
</p>
 
	<p><b>Bio:</b> David Morin is Epic Games’ Industry Manager for Media and Entertainment, where he spearheads efforts from the Unreal Engine team to further its development and adoption in the film and television industries. He also serves as the Executive Director of the Academy Software Foundation, where he works for the Premier Members to develop the use of open source software in the motion picture industry. Additionally, David is chairman of the Joint Technology Committee on Virtual Production, the Joint Technology Committee on Virtual Reality, and a past co-chair of the Joint Technology Subcommittee on Previsualization. He earned a B.Sc.A. in computer science from Laval University (Quebec City, Canada) and has participated in the development of motion capture and 3D software since “Jurassic Park” at companies such as Softimage, Microsoft, Avid Technology, Autodesk, and now Epic Games.</p>
	
</div>




<div class="clear"></div>
